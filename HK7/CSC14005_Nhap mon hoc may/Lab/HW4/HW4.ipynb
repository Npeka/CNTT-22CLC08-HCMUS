{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW4: Neural Network\n",
    "\n",
    "(Cập nhật lần cuối: 23/11/2024)\n",
    "\n",
    "Họ tên: ...\n",
    "\n",
    "MSSV: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nắm cách làm bài và nộp bài"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9889; Bạn lưu ý là mình sẽ dùng chương trình hỗ trợ chấm bài nên bạn cần phải tuân thủ chính xác qui định mà mình đặt ra, nếu không rõ thì hỏi, chứ không nên tự tiện làm theo ý của cá nhân.\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này. Đầu tiên, bạn điền họ tên và MSSV vào phần đầu file ở bên trên. Trong file, bạn làm bài ở những chỗ có ghi là:\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "hoặc đối với những phần code không bắt buộc thì là:\n",
    "```python\n",
    "# YOUR CODE HERE (OPTION)\n",
    "```\n",
    "hoặc đối với markdown cell thì là:\n",
    "```markdown\n",
    "YOUR ANSWER HERE\n",
    "```\n",
    "Tất nhiên, khi làm thì bạn xóa dòng `raise NotImplementedError()` đi.\n",
    "Đối những phần yêu cầu code thì thường ở ngay phía dưới sẽ có một (hoặc một số) cell chứa các bộ test để phần nào giúp bạn biết đã code đúng hay chưa; nếu chạy cell này không có lỗi gì thì có nghĩa là qua được các bộ test. Trong một số trường hợp, các bộ test có thể sẽ không đầy đủ; nghĩa là, nếu không qua được test thì là code sai, nhưng nếu qua được test thì chưa chắc đã đúng hoàn toàn.\n",
    "\n",
    "Trong khi làm bài, bạn có thể cho in ra màn hình, tạo thêm các cell để test. Nhưng khi nộp bài thì bạn xóa các cell mà bạn tự tạo, xóa hoặc comment các câu lệnh in ra màn hình. Bạn lưu ý <font color=red>không được tự tiện xóa các cell hay sửa code của Thầy</font> (trừ những chỗ được phép sửa như đã nói ở trên).\n",
    "\n",
    "Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Nên nhớ mục tiêu chính ở đây là <font color=green>học, học một cách chân thật</font>.  Bạn có thể thảo luận ý tưởng với bạn khác cũng như tham khảo các nguồn trên mạng, nhưng sau cùng <font color=green>code và bài làm phải là của bạn, dựa trên sự hiểu thật sự của bạn</font> (khi tham khảo các nguồn trên mạng thì bạn cần ghi rõ nguồn trong bài làm, và đương nhiên là bạn cũng không được phép đưa code và bài làm cho bạn khác xem). <font color=red>Nếu vi phạm những điều này thì có thể bạn sẽ bị 0 điểm cho toàn bộ môn học.</font>*\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Kernel` - `Restart Kernel & Run All Cells` để restart và chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Kernel` - `Restart Kernel & Run All Cells` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "- Thư mục `MSSV` (ví dụ, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`)\n",
    "    - File `HW4.ipynb` (không cần nộp các file khác)\n",
    "\n",
    "Cuối cùng, bạn nén thư mục `MSSV` này lại với định dạng nén là .zip (chứ không được là .rar hay các định dạng khác) và nộp ở link trên moodle. \\\n",
    "<font color=red>Bạn lưu ý tuân thủ chính xác qui định nộp bài này.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiểm tra môi trường code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn nên thấy kết quả in ra là đường dẫn đến file chạy python của môi trường \"ml-env\" mà mình đã hướng dẫn bạn cài đặt ở HW0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nắm bài toán cần giải quyết của bài tập này\n",
    "\n",
    "Cho dữ liệu quan sát được (đây là dữ liệu huấn luyện gốc, dữ liệu này thường sẽ được tách ra một phần làm dữ liệu validation): \n",
    "$$\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N_{train})}, y^{(N_{train})})\\}$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\textbf{x}^{(n)} \\in \\mathbb{R}^{784}$ là véc-tơ đầu vào chứa các giá trị pixel của một ảnh xám $28\\times28$, ảnh này là ảnh một chữ số viết tay nào đó (véc-tơ $784$ chiều được tạo từ ảnh xám $28\\times28$ bằng cách nối các dòng của ảnh xám lại với nhau)\n",
    "- $y^{(n)} \\in \\{0, 1, ..., 9\\}$ là đầu ra tương ứng, cho biết đây là chữ số nào\n",
    "\n",
    "Nhiệm vụ ở đây là tìm ra một (cách tiền xử lý +) mô hình Neural Network từ dữ liệu này sao cho (cách tiền xử lý +) mô hình Neural Network này có thể nhận đầu vào là một ảnh-chữ-số-viết-tay *mới* (là một véc-tơ $\\in \\mathbb{R}^{784}$) và dự đoán đầu ra tương ứng (chữ số nào trong $\\{0, 1, 2, ..., 9\\}$) một cách *chính xác*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') # Để hình vẽ đẹp hơn một xíu ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: khi tính toán với mảng Numpy, bạn nên dùng các toán-tử/hàm/phương-thức mà Numpy đã cung cấp sẵn. Các toán-tử/hàm/phương-thức này làm trên nguyên mảng và ở bên dưới đã được tối ưu hóa; do đó, code sẽ ngắn gọn và chạy nhanh. Nếu bạn dùng vòng lặp for và làm với từng phần tử của mảng Numpy thì code sẽ dài và chạy chậm $\\to$ bạn sẽ bị trừ điểm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu (giống HW2 & HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bộ dữ liệu mà ta sẽ dùng trong bài này là MNIST - bộ dữ liệu chữ số viết tay \"nổi tiếng\" trong cộng đồng làm Machine Learning. [Bộ MNIST gốc](http://yann.lecun.com/exdb/mnist/) gồm có: dữ liệu huấn luyện (60000 ảnh) và dữ liệu kiểm tra (10000 ảnh). Bộ MNIST mà ta sẽ dùng trong bài này (file \"mnist.pkl.gz\") gồm có: dữ liệu huấn luyện (50000 ảnh), dữ liệu validation (10000 ảnh), và dữ liệu kiểm tra (10000 ảnh); dữ liệu huấn luyện và validation ở đây được tạo ra bằng cách tách ngẫu nhiên dữ luyện huấn luyện gốc ra thành 2 phần theo tỉ lệ 5:1. \n",
    "\n",
    "Về mặt ý nghĩa thì dữ liệu validation và dữ liệu kiểm tra đều là dữ liệu mới ngoài dữ liệu huấn luyện. Dữ liệu validation giống như đề thi thử, có thể được thi một vài lần; còn dữ liệu kiểm tra giống như đề thi thật, để đảm bảo kết quả được khách quan thì chỉ được thi một lần duy nhất! Khi làm Machine Learning, ta thường muốn thử một số cách tiền xử lý + mô hình để chọn ra cách tiền xử lý + mô hình tốt nhất. Với mỗi cách tiền xử lý + mô hình, ta sẽ huấn luyện trên dữ liệu huấn luyện và đo độ lỗi dự đoán trên dữ liệu validation; cuối cùng ta sẽ chọn cách tiền xử lý + mô hình mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation (ta không chọn dựa vào độ lỗi dự đoán trên dữ liệu huấn luyện vì có thể xảy ra trường hợp \"học vẹt\": cách tiền xử lý + mô hình có độ lỗi rất thấp trên dữ liệu huấn luyện nhưng lại có độ lỗi cao với dữ liệu mới ngoài dữ liệu huấn luyện). Khi đã chọn xong cách tiền xử lý + mô hình rồi thì ta sẽ đo một lần duy nhất độ lỗi dự đoán trên dữ liệu kiểm tra để có một ước lượng khách quan về độ lỗi thật sự! Nếu bạn nhìn vào độ lỗi dự đoán trên dữ liệu kiểm tra và quay lại điều chỉnh cách tiền xử lý + mô hình thì kết quả trên dữ liệu kiểm tra sẽ không còn sự khách quan nữa!\n",
    "\n",
    "Đoạn code dưới đây sẽ đọc dữ liệu từ file \"mnist.pkl.gz\" và lưu kết quả vào 6 mảng:\n",
    "\n",
    "- `train_X`, `train_y`\n",
    "- `val_X`, `val_y`\n",
    "- `test_X`, `test_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    if os.path.isfile(mnist_file) == False:\n",
    "        mnist_file = os.path.join(os.path.expanduser('~'), 'data', 'mnist.pkl.gz')\n",
    "    \n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "    train_X, train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data    \n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y, test_X, test_Y\n",
    "\n",
    "# Bạn cần đặt file \"mnist.pkl.gz\" vào cùng thư mục với file notebook này,\n",
    "# hoặc bạn cũng có thể đặt ở thư mục tương ứng với câu lệnh này:\n",
    "# os.path.join(os.path.expanduser('~'), 'data')\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = read_mnist('mnist.pkl.gz')\n",
    "print(f'Shape of train_X: {train_X.shape}, shape of train_y: {train_y.shape}')\n",
    "print(f'Shape of val_X:   {val_X.shape}, shape of val_y:   {val_y.shape}')\n",
    "print(f'Shape of test_X:  {test_X.shape}, shape of test_y:  {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khám phá dữ liệu huấn luyện (giống HW2 & HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta hãy xem thử min và max của `train_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Min of train_X: {train_X.min()}, max of train_X: {train_X.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với ảnh xám thì giá trị của mỗi pixel thường sẽ nằm trong đoạn [0, 255] (với 0 là màu đen và 255 là màu trắng), hoặc đôi khi được chuẩn hóa về đoạn [0, 1] (với 0 là màu đen và 1 là màu trắng). Ở đây có vẻ giá trị pixel của ảnh xám của ta nằm trong đoạn [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta hãy thử xem mặt mũi của vài ảnh trong `train_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bạn có thể chạy cell này nhiều lần để xem các ảnh ngẫu nhiên khác nhau\n",
    "n_rimages = 10; n_cimages = 10 \n",
    "padding = 2 \n",
    "canvas = 0.5 * np.ones((n_rimages * (28 + 2 * padding), n_cimages * (28 + 2 * padding)))\n",
    "rand_idxs = np.random.permutation(np.arange(len(train_X))[:n_rimages * n_cimages])\n",
    "for r in range(n_rimages):\n",
    "    for c in range(n_cimages):\n",
    "        i = r * n_cimages + c\n",
    "        image = train_X[rand_idxs[i]].reshape(28, 28)\n",
    "        temp1 = r * (28 + 2 * padding) + padding \n",
    "        temp2 = c * (28 + 2 * padding) + padding \n",
    "        canvas[temp1:temp1 + 28, temp2:temp2 + 28] = image\n",
    "plt.imshow(canvas, cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta hãy xem các giá trị có thể có của `train_y` và số lượng của mỗi giá trị này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values, counts = np.unique(train_y, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(f'Value: {value}, count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy là `train_y` có 10 giá trị có thể có ứng với 10 chữ số từ 0 đến 9. Và số lượng ảnh của mỗi chữ số cũng khá tương đương nhau. Tốt ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_ones(X):\n",
    "    return np.hstack((np.ones((len(X), 1)), X))\n",
    "\n",
    "# Gọi hàm add_ones để tiền xử lý train_X\n",
    "train_Z = add_ones(train_X)\n",
    "train_Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm mô hình Neural Network từ dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài này, ta sẽ làm với một kiến trúc đơn giản của mô hình Neural Network: Fully-Connected Feed-Forward Neural Network. Ta sẽ dùng hàm kích hoạt sigmoid ở tầng ẩn và hàm softmax ở tầng đầu ra. Như vậy, có thể xem mô hình Neural Network của ta là mô hình Softmax Regression mà có véc-tơ đầu vào là $\\textbf{z}$ với $\\textbf{z}$ được tính từ véc-tơ đầu vào ban đầu $\\textbf{x}$ thông qua các tầng ẩn. Ý tưởng lớn của mô hình Neural Network là mô hình sẽ tự động học luôn cách xác định véc-tơ đầu vào $\\textbf{z}$ từ dữ liệu, thay vì con người phải ngồi suy nghĩ cách thiết kế $\\textbf{z}$. Ưu điểm của cách làm này của mô hình Neural Network là con người có thể để cho máy tính chạy và đi uống coffee :-). Nhưng nhược điểm là mô hình sẽ có nhiều tham số (trọng số) cần phải học hơn, và do đó sẽ cần nhiều dữ liệu hơn để có thể học tốt; điều này cũng có nghĩa là sẽ cần máy tính mạnh hơn, huấn luyện lâu hơn, tốn tiền điện nhiều hơn, thải CO2 ra môi trường nhiều hơn :-(. Một nhược điểm nữa là con người có thể không hiểu được véc-tơ đầu vào $\\textbf{z}$ mà mô hình đưa ra. Về độ chính xác dự đoán, cho đến thời điểm hiện tại mô hình Neural Network là mô hình đạt được độ chính xác dự đoán rất tốt với dữ liệu hình ảnh, âm thanh, văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quay trở lại việc cài đặt mô hình Neural Network trong bài này, ***nhiệm vụ 1 của bạn (2đ):*** viết hàm `compute_nnet_output`. Hàm này sẽ được dùng trong hàm `train_nnet` ở bên dưới; ngoài ra, sau khi huấn luyện xong thì hàm này sẽ được dùng để dự đoán các đầu ra của các véc-tơ đầu vào mới.\n",
    "\n",
    "Hàm `compute_nnet_output` có các tham số đầu vào:\n",
    "- `Ws`: list chứa các mảng trọng số của mô hình Neural Network. `Ws[l-1]` là mảng trọng số của tầng `l` với `l` >= 1 (tầng 0 là tầng đầu vào, không có mảng trọng số); mảng trọng số của tầng `l` có shape là <font color=blue>(</font>1 + số lượng nơ-ron của tầng `l-1`<font color=blue>,</font> số lượng nơ-ron của tầng `l`<font color=blue>)</font> với \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1\n",
    "- `X`: mảng chứa các véc-tơ đầu vào cần dự đoán (đã được thêm 1 ở đầu), mảng này có shape là (`N`, `d+1`) với `N` là số lượng các véc-tơ đầu vào và `d` là số lượng phần tử của mỗi véc-tơ đầu vào (khi chưa thêm 1 ở đầu)\n",
    "- `return_what`: nếu tham số này bằng `all` thì sẽ trả về list chứa các mảng-chứa-các-véc-tơ-đầu-ra ở tất cả các tầng (ta sẽ cần dùng list này trong quá trình huấn luyện); nếu tham số này bằng `prob` thì sẽ trả về mảng chứa các véc-tơ đầu ra ở tầng đầu ra, mỗi véc-tơ đầu ra cho biết xác suất các lớp của véc-tơ đầu vào tương ứng; nếu tham số này bằng `class` hoặc các giá trị khác thì sẽ trả về mảng chứa các lớp đầu ra dự đoán, lớp đầu ra dự đoán của một véc-tơ đầu vào là lớp mà có xác suất lớn nhất trong véc-tơ đầu ra chứa xác suất các lớp\n",
    "\n",
    "Hàm `compute_nnet_output` sẽ trả về:\n",
    "\n",
    "- Nếu `return_what` bằng `all` thì sẽ trả về list chứa các mảng-chứa-các-véc-tơ-đầu-ra ở tất cả các tầng, bao gồm cả tầng 0 (tầng đầu vào). Phần tử chỉ số `i` của list này là mảng chứa các véc-tơ đầu ra của tầng `i` tương ứng với các véc-tơ đầu vào của `X`; mảng này có shape là (`N`, 1 + số lượng nơ-ron của tầng `i`) nếu mảng này không phải là phần tử cuối của list (\"1 + ...\" là ứng với nơ-ron mà luôn có giá trị đầu ra là 1), còn nếu mảng này là phần tử cuối của list (tầng đầu ra) thì sẽ có shape là (`N`, số lượng nơ-ron của tầng `i`). Nhắc lại: \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1, `N` là số lượng véc-tơ đầu vào của `X`\n",
    "- Nếu `return_what` bằng `prob` thì sẽ trả về mảng chứa các véc-tơ đầu ra ở tầng đầu ra mà tương ứng với các véc-tơ đầu vào của `X`, mỗi véc-tơ đầu ra chứa xác suất các lớp của véc-tơ đầu vào tương ứng; mảng này có shape là (`N`, số lượng lớp)\n",
    "- Nếu `return_what` bằng `class` hoặc các giá trị khác thì sẽ trả về mảng chứa các lớp đầu ra dự đoán tương ứng với các véc-tơ đầu vào của `X`, lớp đầu ra dự đoán của một véc-tơ đầu vào là lớp mà có xác suất lớn nhất trong véc-tơ đầu ra chứa xác suất các lớp; mảng này có shape là (`N`,)\n",
    "\n",
    "Trong hàm `compute_nnet_output` thì bạn được phép dùng vòng lặp để duyệt qua các tầng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c1df722b9884b005ae7c9b53b29fae0",
     "grade": false,
     "grade_id": "cell-a7098fc22ed23437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_nnet_output(Ws, X, return_what='class'):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f1f6679f2e85bd8598f35e54188c5bf",
     "grade": true,
     "grade_id": "cell-12f18fd7316e849b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Giả sử ta có mảng X với 4 dòng tương ứng với 4 véc-tơ đầu vào (đã thêm 1 ở đầu)\n",
    "X = np.array([[1.0, 0.9, 0.9], \n",
    "              [1.0, 0.5, 0.4], \n",
    "              [1.0, 0.4, 0.5],\n",
    "              [1.0, 0.1, 0.7]])\n",
    "# Giả sử Neural Network của ta có:\n",
    "# 2 nơ-ron đầu vào - 4 nơ-ron ẩn - 3 nơ-ron ẩn - 2 nơ-ron đầu ra\n",
    "# (không tính nơ-ron mà luôn có giá trị đầu ra là 1)\n",
    "Ws = [np.array([[-0.3 ,  0.2 ,  0.5 , -0.6],\n",
    "                [-0.1 , -0.2 , -0.35,  0.1],\n",
    "                [ 0.45, -0.7 , -0.7 ,  0.9]]),\n",
    "      np.array([[ 0.3 , -0.05,  0.8],\n",
    "                [ 0.6 ,  0.3 , -0.5],\n",
    "                [-0.8 , -0.3 ,  0.5],\n",
    "                [ 0.4 , -0.45,  0.4],\n",
    "                [ 0.1 ,  0.2 , -0.3]]),\n",
    "      np.array([[-0.3 ,  0.6],\n",
    "                [ 0.5 , -0.7],\n",
    "                [-0.45,  0.2],\n",
    "                [ 0.47, -0.2]])]\n",
    "# Kiểm tra hàm compute_nnet_output của bạn!\n",
    "As = compute_nnet_output(Ws, X, 'all')\n",
    "assert len(As) == 4\n",
    "assert As[0].shape == (4, 3)\n",
    "assert As[1].shape == (4, 5)\n",
    "assert As[2].shape == (4, 4)\n",
    "assert As[3].shape == (4, 2)\n",
    "assert str(np.round(As[0][0, 1], 4)) == '0.9'\n",
    "assert str(np.round(As[1][0, 1], 4)) == '0.5037'\n",
    "assert str(np.round(As[2][0, 1], 4)) == '0.6305'\n",
    "assert str(np.round(As[3][0, 1], 4)) == '0.5022'\n",
    "predicted_Y = compute_nnet_output(Ws, X, 'prob')\n",
    "assert predicted_Y.shape == (4, 2)\n",
    "assert str(np.round(predicted_Y[0, 1], 4)) == '0.5022'\n",
    "predicted_y = compute_nnet_output(Ws, X, 'class')\n",
    "assert predicted_y.shape == (4,)\n",
    "assert list(predicted_y) == [1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sau khi đã xong nhiệm vụ 1 thì bây giờ bạn sẽ sang nhiệm vụ 2 và cũng là nhiệm vụ chính của bạn (4đ):*** viết hàm `train_nnet` để huấn luyện mô hình Neural Network với độ lỗi cross-entropy và thuật toán cực tiểu hóa SGD (Stochastic Gradient Descent).\n",
    "\n",
    "Hàm `train_nnet` có các tham số đầu vào:\n",
    "- `X`, `y`, `initial_Ws`, `mb_size`, `lr`, `max_epoch`: giống như ở HW3, mình sẽ không nhắc lại nữa. Một khác biệt nhỏ so với HW3 là `initial_W` được thay bằng `initial_Ws` (có s), vì bây giờ ta không chỉ có một mảng trọng số mà là một list các mảng trọng số. Ngoài ra, so với HW3 thì ở đây ta chỉ dùng `max_epoch` để ngắt SGD, chứ không có `wanted_mbe` (ở HW3, ta đưa thêm `wanted_mbe` chủ yếu là để so sánh GD và SGD)\n",
    "- `hid_layer_sizes`: list cho biết số lượng nơ-ron của các tầng ẩn (không tính nơ-ron mà luôn có giá trị đầu ra là 1); ví dụ, list `[20, 10]` nghĩa là Neural Network của ta có 2 tầng ẩn, trong đó tầng ẩn thứ nhất có 20 nơ-ron, tầng ẩn thứ hai có 10 nơ-ron\n",
    "\n",
    "Hàm `train_nnet` trả về:\n",
    "- List chứa các mảng trọng số của mô hình Neural Network. Phần tử chỉ số `l-1` của list này là mảng trọng số của tầng `l` với `l` >= 1 (tầng 0 là tầng đầu vào, không có mảng trọng số); mảng trọng số của tầng `l` có shape là <font color=blue>(</font>1 + số lượng nơ-ron của tầng `l-1`<font color=blue>,</font> số lượng nơ-ron của tầng `l`<font color=blue>)</font> với \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1\n",
    "- List chứa độ lỗi cross-entropy trên dữ liệu huấn luyện sau mỗi epoch (giống HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như đã nói, để tìm các trọng số của mô hình Neural Network thì ta sẽ dùng thuật toán SGD để cực tiểu độ lỗi cross-entropy trên dữ liệu huấn luyện. Như vậy, về cơ bản là hàm `train_nnet` giống như hàm `train_smreg` ở HW3 (ở đây ta cũng thống nhất là sẽ sử dụng cách cài đặt bước \"xáo trộn ngẫu nhiên các mẫu dữ liệu huấn luyện\" và cách xử lý phần dữ liệu bị lẻ ở cuối giống như ở HW3). Có một điểm khác là ở mỗi mini-batch, ta không chỉ phải cập nhật cho các trọng số của mô hình Softmax Regression ứng với tầng đầu ra, mà còn phải cập nhật cho các trọng số của các tầng ẩn trước đó. \n",
    "\n",
    "Như đã biết, trong SGD, để cập nhật cho một trọng số thì ta cần tính trung bình trên mini-batch đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo trọng số này. Xét trọng số nối từ nơ-ron i của tầng l-1 tới nơ-ron j của tầng l. Sau khi áp dụng \"chain rule\" (chi tiết bạn có thể xem ở trang 10-12 của file \"HW4-Slide.pdf\"), ta có công thức để tính đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo trọng số này:\n",
    "\n",
    "Đạo hàm riêng <font color=blue>=</font> giá trị đầu ra của nơ-ron i ở tầng l-1 <font color=blue>*</font> delta của nơ-ron j ở tầng l\n",
    "\n",
    "Với delta của một nơ-ron là đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo tổng có trọng số (giá trị tính được ngay trước khi áp dụng hàm kích hoạt) của nơ-ron này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là mã giả của thuật toán SGD để huấn luyện mô hình Neural Network.\n",
    "\n",
    "===\n",
    "\n",
    "Khởi tạo các trọng số (lưu ý: với Neural Network, cách khởi tạo trọng số sẽ ảnh hưởng nhiều đến kết quả tìm được bởi SGD)\n",
    "\n",
    "Với mỗi epoch (một epoch ám chỉ một lần duyệt qua toàn bộ các mẫu dữ liệu huấn luyện):\n",
    "- Xáo trộn ngẫu nhiên các mẫu dữ liệu huấn luyện\n",
    "- Chia các mẫu dữ liệu huấn luyện ra thành các mini-batch có kích thước bằng nhau (kích thước này do người dùng chỉ định). Với mỗi mini-batch:\n",
    "    1. Thực hiện lan truyền tiến các véc-tơ đầu vào qua Neural Network để có được các véc-tơ đầu ra ở tất cả các tầng\n",
    "    2. Tính các véc-tơ delta của tầng cuối từ các véc-tơ đầu ra của tầng cuối (véc-tơ chứa xác suất các lớp) và các véc-tơ đầu ra đúng của mini-batch (véc-tơ one-hot) \n",
    "    3. Tính gradient (các đạo riêng) của tầng cuối từ các véc-tơ đầu ra của tầng kế cuối và các véc-tơ delta của tầng cuối\n",
    "    4. Cập nhật các trọng số của tầng cuối dựa trên gradient\n",
    "    5. Duyệt từ tầng kế cuối về đầu. Với tầng i:\n",
    "        1. Tính các véc-tơ delta của tầng i từ các véc-tơ delta của tầng i+1, các trọng số của tầng i+1, và các véc-tơ đầu ra của tầng i\n",
    "        2. Tính gradient (các đạo riêng) của tầng i từ các véc-tơ đầu ra của tầng i-1 và các véc-tơ delta của tầng i\n",
    "        3. Cập nhật các trọng số của tầng i dựa trên gradient\n",
    "\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mã giả ở trên, bạn đã cài đặt bước 1 ứng với hàm `compute_nnet_output` rồi. Bây giờ mình sẽ đưa ra các ví dụ cụ thể cho các bước 2, 3, 5A, 5B; dựa vào các ví dụ cụ thể này, bạn có thể kiểm tra xem code của bạn cho các bước này đã đúng hay chưa. Nói thêm: thật ra bạn đã làm bước 2 và 3 khi cài đặt mô hình Softmax Regression rồi, nhưng lúc đó bạn không tách ra bước 2.\n",
    "\n",
    "**Bước 2.** Tính các véc-tơ delta của tầng cuối từ các véc-tơ đầu ra của tầng cuối (véc-tơ chứa xác suất các lớp) và các véc-tơ đầu ra đúng của mini-batch (véc-tơ one-hot)\n",
    "\n",
    "Giả sử ta có 3 lớp và kích thước của mini-batch là 2. Gọi `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng cuối (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch, và `mb_Y` là mảng 2 chiều chứa các véc-tơ đầu ra đúng của mini-batch (véc-tơ đầu ra đúng là véc-tơ one-hot, mỗi véc-tơ là một dòng của mảng). Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[0.8, 0.7, 0.6],\n",
    "              [0.5, 0.6, 0.5]]) \n",
    "mb_Y = np.array([[0, 1, 0],\n",
    "                 [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì mảng 2 chiều chứa các véc-tơ delta của tầng cuối (mỗi véc-tơ là một dòng của mảng) sẽ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array([[ 0.8, -0.3,  0.6],\n",
    "          [-0.5,  0.6,  0.5]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể xem công thức tính delta của tầng cuối ở trang 14 của file \"HW4-Slide.pdf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 3.** Tính gradient (các đạo riêng) của tầng cuối từ các véc-tơ đầu ra của tầng kế cuối và các véc-tơ delta của tầng cuối\n",
    "\n",
    "Giả sử ta có 3 lớp (cũng tức là tầng cuối có 3 nơ-ron), tầng kế cuối có 4 nơ-ron (không tính nơ-ron mà luôn có giá trị đầu ra là 1), và kích thước của mini-batch là 2. Gọi `delta` là mảng 2 chiều chứa các véc-tơ delta của tầng cuối (mỗi véc-tơ là một dòng của mảng) ứng với các mẫu dữ liệu huấn luyện của mini-batch (mẫu dữ liệu huấn luyện gồm véc-tơ đầu vào và đầu ra đúng), và `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng kế cuối (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch. Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                  [-0.5,  0.6,  0.5]])\n",
    "A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "              [1.0, 0.9, 0.8, 0.7, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì gradient của tầng cuối (gradient là mảng có cùng shape với mảng trọng số của tầng cuối) sau khi gọi phương thức `round(4)` (làm tròn 4 chữ số thập phân) sẽ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array([[ 0.15 ,  0.15 ,  0.55 ],\n",
    "          [-0.025,  0.195,  0.375],\n",
    "          [-0.16 ,  0.225,  0.23 ],\n",
    "          [-0.055,  0.165,  0.265],\n",
    "          [ 0.055,  0.   ,  0.085]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gợi ý: ở trên mình đã có đưa ra công thức để tính đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo một trọng số; với một mini-batch thì bạn sẽ dùng công thức này để tính đạo hàm riêng với từng mẫu dữ liệu huấn luyện trong mini-batch, rồi sau cùng là lấy trung bình lại. \n",
    "\n",
    "Nếu thấy khó quá thì bạn có thể đoán cách tính gradient từ ví dụ ở trên và hy vọng ... Để củng cố hy vọng, bạn có thể đối chiếu với cách tính gradient của mô hình Softmax Regression mà bạn đã cài đặt ở HW3; như mình đã có nói, thật ra là bạn đã cài đặt bước 2 và 3 ở HW3, nhưng lúc đó là bạn không tách bước 2 ra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 5A.** Tính các véc-tơ delta của tầng i từ các véc-tơ delta của tầng i+1, các trọng số của tầng i+1, và các véc-tơ đầu ra của tầng i (lưu ý là ta không tính delta của nơ-ron mà luôn có giá trị đầu ra là 1) \n",
    "\n",
    "Giả sử tầng i có 4 nơ-ron, tầng i+1 có 3 nơ-ron (không tính nơ-ron mà luôn có giá trị đầu ra là 1), kích thước của mini-batch là 2. Gọi `delta` là mảng 2 chiều chứa các véc-tơ delta ở tầng i+1 (mỗi véc-tơ là một dòng của mảng) ứng với các mẫu dữ liệu huấn luyện của mini-batch, `W` là mảng 2 chiều chứa các trọng số của tầng i+1, `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng i (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch. Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                  [-0.5,  0.6,  0.5]])\n",
    "W = np.array([[-0.75,  0.95,  0.15],\n",
    "              [ 0.25,  0.29,  0.97],\n",
    "              [-0.56,  0.52,  0.73],\n",
    "              [ 0.05,  0.66,  0.46],\n",
    "              [-0.05,  0.  , 0.08]])\n",
    "A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "              [1.0, 0.9, 0.8, 0.7, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì mảng 2 chiều chứa các véc-tơ delta của tầng i (mỗi véc-tơ là một dòng của mảng) sẽ được tính như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta.dot(W.T[:, 1:]) * A[:, 1:] * (1 - A[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do bước này hơi khó nên mình tiết lộ code luôn rồi đó ;-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 5B.** Tính gradient (các đạo riêng) của tầng i từ các véc-tơ đầu ra của tầng i-1 và các véc-tơ delta của tầng i\n",
    "\n",
    "Tương tự như bước 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước khi bạn viết hàm `train_nnet` thì mình nói thêm ý cuối cùng này nữa. Trong hàm `train_nnet` thì bạn được phép dùng vòng lặp để duyệt qua các epoch, trong mỗi epoch thì bạn được phép dùng vòng lặp để duyệt qua các mini-batch, trong mỗi mini-batch thì bạn được phép dùng vòng lặp để duyệt các tầng của Neural Network. Các bước 2, 3, 4, 5A, 5B, 5C trong mã giả ở trên đều có thể được cài đặt chỉ với một dòng code cho mỗi bước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3057561f48f70c66600d99d73460bde5",
     "grade": false,
     "grade_id": "cell-aa53d3e85ae3e1ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nnet(X, y, \n",
    "               hid_layer_sizes, \n",
    "               initial_Ws, mb_size, lr, max_epoch):\n",
    "    # Cách để khởi tạo tốt các trọng số khá là \"bí hiểm\"\n",
    "    # nên mình sẽ làm cho bạn ;-)\n",
    "    np.random.seed(0) # Cố định sự ngẫu nhiên để mình và bạn sẽ ra cùng kết quả\n",
    "    n_classes = len(np.unique(y)) \n",
    "    if initial_Ws is None:\n",
    "        layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [n_classes]\n",
    "        Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) \n",
    "              / np.sqrt(layer_sizes[i] + 1) \n",
    "              for i in range(len(layer_sizes) - 1)] \n",
    "    else:\n",
    "        Ws = initial_Ws\n",
    "    \n",
    "    # Phần còn lại là của bạn\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aad1c4a6a216a6258646e2e81b8b26bd",
     "grade": true,
     "grade_id": "cell-02856e5a3b967345",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Ws, ces = train_nnet(train_Z, train_y, \n",
    "                    hid_layer_sizes=[20], \n",
    "                    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=2)\n",
    "assert len(Ws) == 2\n",
    "assert Ws[0].shape == (785, 20)\n",
    "assert Ws[1].shape == (21, 10)\n",
    "assert str(Ws[0][0, 0].round(4)) == '0.15'\n",
    "assert str(Ws[1][0, 0].round(4)) == '0.1992'\n",
    "assert len(ces) == 2\n",
    "assert str(np.round(ces[0], 4)) == '0.285'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở trên, ta chỉ test nhỏ nhỏ để nhanh chóng kiểm tra tính đúng đắn của hàm `train_nnet`. Bây giờ, ta mới làm thật: gọi hàm `train_nnet` để tìm các trọng số của mô hình Neural Network từ `train_Z` và `train_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Ws_1, train_ces = train_nnet(\n",
    "    train_Z, train_y,\n",
    "    hid_layer_sizes=[50], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=100)\n",
    "plt.plot(np.log(train_ces))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log (for clarity) of training cross-entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng mô hình Neural Network tìm được để dự đoán với dữ liệu huấn luyện và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mbe(predicted_y, y):\n",
    "    return np.mean(predicted_y != y) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_train_y = compute_nnet_output(Ws_1, train_Z, return_what='class')\n",
    "train_mbe = compute_mbe(predicted_train_y, train_y)\n",
    "assert str(np.round(train_mbe, 4)) == '0.002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở HW3, độ lỗi MBE trên dữ liệu huấn luyện của mô hình Softmax Regression (+ cách tiền xử lý đi kèm) tốt nhất là 4.x%. Với mô hình Neural Network ở bài này, độ lỗi MBE trên dữ liệu huấn luyện đã giảm xuống 0.002% :-O.\n",
    "\n",
    "Nhưng ta khoan hãy vui mừng, vì độ lỗi siêu thấp trên dữ liệu huấn luyện có thể là do \"học vẹt\". Ta hãy xem độ lỗi trên dữ liệu validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_Z = add_ones(val_X)\n",
    "predicted_val_y = compute_nnet_output(Ws_1, val_Z, return_what='class')\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "assert str(np.round(val_mbe, 4)) == '2.89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Độ lỗi MBE trên dữ liệu validation cũng giảm theo (mặc dù không giảm nhiều như trên dữ liệu huấn luyện): từ 4.x% với mô hình Softmax Regression tốt nhất ở HW3 giảm xuống 2.89%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đưa ra ý tưởng cải tiến"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình Neural Network hiện tại có độ lỗi trên dữ liệu huấn luyện nhỏ hơn khá nhiều so với độ lỗi trên dữ liệu validation (0.002% vs 2.89%). Đây là dấu hiệu cho thấy mô hình Neural Network hiện tại có thể đang bị \"overfitting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a46ddd6996296ddd044b2b1d21d0867a",
     "grade": false,
     "grade_id": "cell-134c34ab9e8431ce",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "***Nhiệm vụ của bạn (2đ):*** tìm hiểu về một phương pháp chống \"overfitting\" cho Neural Network và trình bày ở bên dưới để người đọc có thể hiểu được ý tưởng của phương pháp này (phương pháp này làm gì? tại sạo làm như vậy lại giúp chống \"overfitting\"?). Gợi ý tên một số phương pháp: \"weight decay (L2 regularization)\", \"max norm contraint\", \"drop-out\", \"data augmentation\", ... (bạn chỉ cần chọn một phương pháp thôi). Khi trình bày, nếu bạn có tham khảo ở đâu thì bạn nhớ ghi rõ nguồn. Bạn lưu ý mục tiêu ở đây là rèn luyện việc trình bày với người đọc về những gì bạn hiểu một cách rõ ràng và chân thật (chứ không phải là bưng nguyên thông tin ở một nguồn nào đó vô)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE \\\n",
    "(Nếu muốn thì bạn có thể chèn thêm các markdown cell để trình bày)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "355e87554f43c626b8f36de591a44635",
     "grade": false,
     "grade_id": "cell-2054bd26c1337331",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "***Nhiệm vụ kế tiếp của bạn (2đ):*** cài đặt phương pháp chống \"overfitting\" mà bạn đã tìm hiểu! Ở bên dưới, mình đã để sẵn các mục quen thuộc cho bạn, với mỗi mục thì bạn có thể tùy ý chèn thêm các cell để làm. \n",
    "\n",
    "Nếu phương pháp chống \"overfitting\" của bạn không liên quan đến việc tiền xử lý dữ liệu thì bạn có thể bỏ qua mục \"Tiền xử lý dữ liệu huấn luyện\"; khi huấn luyện thì có thể dùng lại mảng `train_Z` (là mảng `train_X` được thêm 1 ở đầu) mà đã tạo ở trên. \n",
    "\n",
    "Ở mục \"Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả\", bạn nhớ in ra độ lỗi MBE trên dữ liệu validation và so sánh với độ lỗi MBE hiện tại (2.89%); trong trường hợp kết quả không được cải thiện, bạn thử suy nghĩ xem tại sao lại như vậy, nếu không biết tại sao thì bạn cứ nói là không biết tại sao. \n",
    "\n",
    "Hai mục sau cùng (\"Chọn ra cách tiền xử lý + mô hình Neural Network sau cùng là cách tiền xử lý + mô hình Neural Network mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation\" và \"Dùng cách tiền xử lý + mô hình Neural Network sau cùng để đi thi thật!\") là hai mục không bắt buộc (không có điểm) nhưng bạn nên làm cho đầy đủ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm mô hình Neural Network từ dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng mô hình Neural Network tìm được để dự đoán với dữ liệu huấn luyện và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chọn ra cách tiền xử lý + mô hình Neural Network sau cùng là cách tiền xử lý + mô hình Neural Network mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng cách tiền xử lý + mô hình Neural Network sau cùng để đi thi thật!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "BT05-NeuralNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "509px",
    "left": "0px",
    "right": "1212px",
    "top": "106px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
